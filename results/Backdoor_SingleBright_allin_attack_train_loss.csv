epoch,training_loss
1,1.911178809762001
2,1.911050724585851
3,1.911050727089246
4,1.9110507255395253
5,1.9110507249037425
6,1.9110507260163625
7,1.9110507275660833
8,1.9110507244666417
9,1.9110507258574168
10,1.9110507268905639
11,1.911050726691882
12,1.9110507222016653
13,1.9110507272879282
14,1.9110507276455562
15,1.911050723751386
16,1.9110507287184397
17,1.9110507255395253
18,1.9110507276455562
19,1.9110507243474324
20,1.9110507248242696
21,1.9110507283608118
22,1.9110507283608118
23,1.9110507279237112
24,1.911050726612409
25,1.9110507260163625
26,1.911050728559494
27,1.9110507272481918
28,1.9110507252613704
29,1.9110507253805797
30,1.9110507262945176
31,1.911050725698471
32,1.911050726334254
33,1.911050724585851
34,1.9110507258574168
35,1.9110507282416025
36,1.9110507260958354
37,1.911050723195076
38,1.911050726334254
39,1.9110507287184397
40,1.9110507272879282
41,1.9110507255395253
42,1.911050726334254
43,1.9110507264137269
44,1.9110507272481918
45,1.9110507262945176
46,1.9110507244666417
47,1.911050728201866
48,1.9110507271289825
49,1.9110507285992304
50,1.9110507281223932
